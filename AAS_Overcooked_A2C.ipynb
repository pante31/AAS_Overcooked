{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S11qsP7bl8Qm"
      },
      "source": [
        "# **_Autonomous and Adaptive Systems_ - 2025**\n",
        "## **Mini-Project**:*Overcooked* - A2C implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5Fad_rzpDFx"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-18 10:31:35.341948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, Overcooked\n",
        "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
        "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
        "from overcooked_ai_py.planning.planners import MediumLevelActionManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing _Overcooked_ environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `base_mdp` = An MDP grid world based off of the Overcooked game.\n",
        "- `base_env` = An environment wrapper for the OvercookedGridworld Markov Decision Process. The environment keeps track of the current state of the agent, updates it as the agent takes actions, and provides rewards to the agent.\n",
        "- `env`= Similar to gym env."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Evn4sHxIrX4c"
      },
      "outputs": [],
      "source": [
        "base_mdp = OvercookedGridworld.from_layout_name(\"cramped_room\", old_dynamics = True) # or other layout\n",
        "base_env = OvercookedEnv.from_mdp(base_mdp, info_level=0, horizon=400)\n",
        "env = Overcooked(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are different experimetnal layouts:\n",
        "- ***Cramped Room*** presents low-level coordination challenges: in this shared, confined space it is very easy for the agents to collide.\n",
        "- ***Asymmetric Advantages*** tests whether players can choose high-level strategies that play to their strengths.\n",
        "- ***Coordination Ring***, players must coordinate to travel between the bottom left and top right corners of the layout.\n",
        "- ***Forced Coordination*** removes collision coordination problems, and forces players to develop a high-level joint strategy, since neither player can serve a dish by themselves.\n",
        "- ***Counter Circuit*** involves a non-obvious coordination strategy, where onions are passed over the counter to the pot, rather than being carried around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Actions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The possible actions are: _up, down, left, right, noop,_ and _\"interact\"_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The action space has dimension: Discrete(6)\n"
          ]
        }
      ],
      "source": [
        "print('The action space has dimension: {}'.format(env.action_space))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Observations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'both_agent_obs': (array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
              "          2.,  0.,  0.,  0.,  0.,  2.,  2.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  1.,  0.,  0.,  0., -2.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
              "          2.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
              "          0.,  2.,  0.,  1.,  1.]),\n",
              "  array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0., -2.,\n",
              "          2.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0., -1.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  2.,\n",
              "          2.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
              "          1., -2.,  0.,  3.,  1.])),\n",
              " 'overcooked_state': <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState at 0x105167c70>,\n",
              " 'other_agent_env_idx': 1}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "action = env.action_space.sample()\n",
        "observation, reward, done, info = env.step((action, action))\n",
        "observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIughLCjrUuR"
      },
      "source": [
        "### Understanding the _Overcooked_ **observations** to apply shaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `[0:4]` pi_orientation: length 4 one-hot-encoding of direction currently facing\n",
        "- `[4:8]` pi_obj: length 4 one-hot-encoding of object currently being held (all 0s if no object held) (onion|soup|dish|tomato)\n",
        "- `[8:20]` pi_closest_{onion|tomato|dish|soup|serving|empty_counter}: (dx, dy) where dx = x dist to item, dy = y dist to item. (0, 0) if item is currently held\n",
        "- `[20:22]` pi_cloest_soup_n_{onions|tomatoes}: int value for number of this ingredient in closest soup ???\n",
        "- `[22:23]` pi_closest_pot_{j}_exists: {0, 1} depending on whether jth closest pot found. If 0, then all other pot features are 0. Note: can be 0 even if there are more than j pots on layout, if the pot is not reachable by player i\n",
        "- `[23:27]` pi_closest_pot_{j}_{is_empty|is_full|is_cooking|is_ready}: {0, 1} depending on boolean value for jth closest pot\n",
        "- `[27:29]` pi_closest_pot_{j}_{num_onions|num_tomatoes}: int value for number of this ingredient in jth closest pot\n",
        "- `[29:30]` pi_closest_pot_{j}_cook_time: int value for seconds remaining on soup. -1 if no soup is cooking\n",
        "- `[30:32]` pi_closest_pot_{j}: (dx, dy) to jth closest pot from player i location\n",
        "- `[32:36]` pi_wall: length 4 boolean value of whether player i has wall in each direction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OvercookedRewardShaping(Overcooked):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def step(self, actions):\n",
        "        observation, base_reward, done, info = super().step(actions)\n",
        "        if base_reward != 0:\n",
        "            print(\"Soup delivered! Voto: {}\".format(base_reward)) # base_reward is 20 if soup is delivered\n",
        "        shaped_reward = base_reward + self._compute_shaping(observation['both_agent_obs'])\n",
        "        return observation, shaped_reward, done, info\n",
        "\n",
        "    def _compute_shaping(self, observations):\n",
        "        shaping = 0\n",
        "        for obs in observations:\n",
        "            holding_vector = obs[4:8]\n",
        "            holding_soup = obs[5:6]\n",
        "            soup_full_cooking_ready = obs[24:27]\n",
        "            soup_empty = obs[23:24]\n",
        "            soup_cooking = obs[25:26]\n",
        "            pot_onions = obs[27:28]\n",
        "            \n",
        "            # Penalty if holding an object\n",
        "            #if holding_vector.any():\n",
        "            #    shaping -= 0.05\n",
        "            # Reward if holding a soup\n",
        "            #if holding_soup.any():\n",
        "            #    shaping += 0.1\n",
        "            # Reward if soup is full/cooking/ready\n",
        "            if soup_cooking.any():\n",
        "                shaping += 0.3\n",
        "            # Reward if onion are putted into the soup\n",
        "            #if soup_empty.any():\n",
        "            # shaping += int(pot_onions)*0.01\n",
        "\n",
        "        return shaping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = OvercookedRewardShaping(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing a random episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i_episode in range(1):\n",
        "    observation = env.reset()\n",
        "    \n",
        "    for t in range(100):\n",
        "        action = env.action_space.sample()\n",
        "        state = env.step((action, action))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_inputs = 96 # length of the observation array\n",
        "num_actions = 6\n",
        "num_hidden = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### _Actor_ model\n",
        "- **Actor**: This takes as input the _state of our environment_ and returns a _probability value_ for each action in its action space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actor-Critic for Player 1\n",
        "actor = keras.models.Sequential([\n",
        "    keras.layers.Input(shape=(num_inputs,)),\n",
        "    keras.layers.Dense(num_hidden, activation='relu'),\n",
        "    keras.layers.Dense(num_hidden, activation='relu'),\n",
        "    keras.layers.Dense(num_hidden, activation='relu'),\n",
        "    keras.layers.Dense(num_actions, activation='softmax')\n",
        "])\n",
        "\n",
        "actor = keras.models.Sequential([\n",
        "    keras.layers.Input(shape=(num_inputs,)),\n",
        "    keras.layers.Dense(num_hidden//2, activation='relu'),\n",
        "    keras.layers.Dense(num_hidden//2, activation='relu'),\n",
        "    keras.layers.Dense(num_actions, activation='softmax')\n",
        "])\n",
        "\n",
        "actor_optimizer = keras.optimizers.Adam(learning_rate=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### _Critic_ model\n",
        "- **Critic**: This takes as input the _state of our environment_ and returns an estimate of _total rewards_ in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "critic = keras.models.Sequential([\n",
        "    keras.layers.Input(shape=(num_inputs,)),\n",
        "    keras.layers.Dense(num_hidden, activation='relu'),\n",
        "    keras.layers.Dense(num_hidden, activation='relu'),\n",
        "    keras.layers.Dense(num_hidden//2, activation='relu'),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "critic = keras.models.Sequential([\n",
        "    keras.layers.Input(shape=(num_inputs,)),\n",
        "    keras.layers.Dense(num_hidden//2, activation='relu'),\n",
        "    keras.layers.Dense(num_hidden//4, activation='relu'),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "critic_optimizer = keras.optimizers.Adam(learning_rate=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sources: \n",
        "- https://medium.com/data-science-in-your-pocket/advantage-actor-critic-a2c-algorithm-in-reinforcement-learning-with-codes-and-examples-using-e810273c0c9e\n",
        "- https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration parameters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n",
        "mse_loss = keras.losses.MeanSquaredError()\n",
        "\n",
        "num_episodes = 10\n",
        "n_step_before_update = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <u>Update</u>: After each **episode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chef1_action_probs_history = []\n",
        "chef1_critic_value_history = []\n",
        "chef2_action_probs_history = []\n",
        "chef2_critic_value_history = []\n",
        "rewards_history = []\n",
        "\n",
        "running_reward = 0\n",
        "\n",
        "#while True:  # Run until solved\n",
        "for episode in tqdm(range(num_episodes)):\n",
        "    observation = env.reset() #observation of the starting state\n",
        "    episode_reward = 0\n",
        "    \n",
        "    with tf.GradientTape(persistent = True) as tape:\n",
        "        while True:\n",
        "\n",
        "            chef1_observation = observation['both_agent_obs'][0]\n",
        "            chef2_observation = observation['both_agent_obs'][1]\n",
        "\n",
        "            #chef1_observation = keras.ops.convert_to_tensor([chef1_observation])\n",
        "            chef1_observation = keras.ops.convert_to_tensor(chef1_observation)\n",
        "            chef1_observation = keras.ops.expand_dims(chef1_observation, 0)\n",
        "\n",
        "            chef2_observation = keras.ops.convert_to_tensor(chef2_observation)\n",
        "            chef2_observation = keras.ops.expand_dims(chef2_observation, 0)\n",
        "\n",
        "            # Predict action probabilities and estimated future rewards\n",
        "            # from environment state\n",
        "            chef1_action_probs = actor(chef1_observation)\n",
        "            chef1_critic_value = critic(chef1_observation)\n",
        "            chef1_critic_value_history.append(chef1_critic_value[0, 0])\n",
        "\n",
        "            chef2_action_probs = actor(chef2_observation)\n",
        "            chef2_critic_value = critic(chef2_observation)\n",
        "            chef2_critic_value_history.append(chef2_critic_value[0, 0])\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            chef1_action = np.random.choice(num_actions, p=np.squeeze(chef1_action_probs))\n",
        "            chef1_action_probs_history.append(keras.ops.log(chef1_action_probs[0, chef1_action]))\n",
        "\n",
        "            chef2_action = np.random.choice(num_actions, p=np.squeeze(chef2_action_probs))\n",
        "            chef2_action_probs_history.append(keras.ops.log(chef2_action_probs[0, chef2_action]))\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            next_observation, reward, done, info = env.step((chef1_action, chef2_action))\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward to check condition for solving\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Calculate expected value from rewards\n",
        "        # - At each timestep what was the total reward received after that timestep\n",
        "        # - Rewards in the past are discounted by multiplying them with gamma\n",
        "        # - These are the labels for our critic\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "\n",
        "        # Normalize\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Calculating loss values to update the networks - PLAYER 1 #\n",
        "        chef1_history = zip(chef1_action_probs_history, chef1_critic_value_history, returns)\n",
        "        chef1_actor_losses = []\n",
        "        chef1_critic_losses = []\n",
        "\n",
        "        for log_prob, value, ret in chef1_history:\n",
        "            # At this point in history, the critic estimated that we would get a\n",
        "            # total reward = `value` in the future. We took an action with log probability\n",
        "            # of `log_prob` and ended up receiving a total reward = `ret`.\n",
        "            # The actor must be updated so that it predicts an action that leads to\n",
        "            # high rewards (compared to critic's estimate) with high probability.\n",
        "            diff = ret - value\n",
        "            chef1_actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "            # The critic must be updated so that it predicts a better estimate of\n",
        "            # the future rewards.\n",
        "            chef1_critic_losses.append(\n",
        "                mse_loss(keras.ops.expand_dims(value, 0), keras.ops.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # Calculating loss values to update the networks - PLAYER 2 #\n",
        "        chef2_history = zip(chef2_action_probs_history, chef2_critic_value_history, returns)\n",
        "        chef2_actor_losses = []\n",
        "        chef2_critic_losses = []\n",
        "\n",
        "        for log_prob, value, ret in chef2_history:\n",
        "            # At this point in history, the critic estimated that we would get a\n",
        "            # total reward = `value` in the future. We took an action with log probability\n",
        "            # of `log_prob` and ended up receiving a total reward = `ret`.\n",
        "            # The actor must be updated so that it predicts an action that leads to\n",
        "            # high rewards (compared to critic's estimate) with high probability.\n",
        "            diff = ret - value\n",
        "            chef2_actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "            # The critic must be updated so that it predicts a better estimate of\n",
        "            # the future rewards.\n",
        "            chef2_critic_losses.append(\n",
        "                mse_loss(keras.ops.expand_dims(value, 0), keras.ops.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # Summing up all the losses\n",
        "        actor_loss_value = sum(chef1_actor_losses) + sum(chef2_actor_losses)\n",
        "        critic_loss_value = sum(chef1_critic_losses) + sum(chef2_critic_losses)\n",
        "        \n",
        "    # Backpropagation for both Actor & Critic   \n",
        "    actor_grads = tape.gradient(actor_loss_value, actor.trainable_variables)\n",
        "    critic_grads = tape.gradient(critic_loss_value, critic.trainable_variables)\n",
        "    \n",
        "    actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
        "    critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
        "\n",
        "    # Clear the loss and reward history\n",
        "    chef1_action_probs_history.clear()\n",
        "    chef1_critic_value_history.clear()\n",
        "    chef2_action_probs_history.clear()\n",
        "    chef2_critic_value_history.clear()\n",
        "    rewards_history.clear()\n",
        "        \n",
        "    del tape # remove the reference to the tape and invoke garbage collection\n",
        "\n",
        "    # Log details\n",
        "    if (episode + 1) % 5 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_reward, episode+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "50 episodes at the time (learning rate = 5e-7)\n",
        "    - UP --> 1.6-7, soup: 2\n",
        "             2-10.7, soup: 6 \n",
        "    - DOWN --> 0.5-4.5, soup: 3\n",
        "               2-4, soup: 2\n",
        "100 episodes at the time (learning rate = 5e-7)\n",
        "    - UP --> 0.5-6.6, soup: 0\n",
        "             1.5-7.6, soup: 4\n",
        "             1-7.5, soup: 1\n",
        "300 episodes at the time (learning rate = 5e-8)\n",
        "    - DOWN --> 0.5-6.5, soup: 18 \n",
        "300 episodes at the time (learning rate = 5e-9)\n",
        "    - DOWN --> 0.5-6.5, soup: 18 \n",
        "\n",
        "700 episodes - 128 net_dimensions - 1e-4 --> No learning at all\n",
        "700 episodes - 128 net_dimensions - 1e-5 --> No learning at all\n",
        "\n",
        "100 episodes - 64 net_dimensions - 1e-6 --> No learning at all\n",
        "100 episodes - 64 net_dimensions - 1e-7 --> No learning at all\n",
        "\n",
        "100 episodes - 256 net_dimensions - 1e-5 --> No learning at all, it goes to 0\n",
        "180 episodes - 256 net_dimensions - 1e-8 --> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <u>Update</u>: After each **step**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [03:26<05:10, 51.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: 1.49 at episode 4 Actor Loss: -0.16 - Critic Loss: 0.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 9/10 [08:12<00:57, 57.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: 1.62 at episode 9 Actor Loss: -0.09 - Critic Loss: 0.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [09:08<00:00, 54.84s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(close=None, block=None)>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "running_reward = 0\n",
        "reward_histroy = []\n",
        "\n",
        "actor_optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "critic_optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "losses = {'Actor Loss': [], 'Critic Loss': []}\n",
        "\n",
        "#while True:  # Run until solved\n",
        "for episode in tqdm(range(1, num_episodes+1)):\n",
        "    observation = env.reset() #observation of the starting state\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        with tf.GradientTape(persistent = True) as tape:\n",
        "\n",
        "            chef1_observation = observation['both_agent_obs'][0]\n",
        "            chef2_observation = observation['both_agent_obs'][1]\n",
        "\n",
        "            #chef1_observation = keras.ops.convert_to_tensor([chef1_observation])\n",
        "            chef1_observation = tf.convert_to_tensor(chef1_observation, dtype = tf.float32)\n",
        "            chef1_observation = keras.ops.expand_dims(chef1_observation, 0)\n",
        "\n",
        "            chef2_observation = tf.convert_to_tensor(chef2_observation, dtype = tf.float32)\n",
        "            chef2_observation = keras.ops.expand_dims(chef2_observation, 0)\n",
        "\n",
        "            # Predict action probabilities and estimate future rewards\n",
        "            # from environment state (actor & critic networks)\n",
        "            chef1_action_probs = actor(chef1_observation)\n",
        "            chef1_observation_value = critic(chef1_observation)\n",
        "\n",
        "            chef2_action_probs = actor(chef2_observation)\n",
        "            chef2_observation_value = critic(chef2_observation)\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            chef1_action_probs_dist = tfp.distributions.Categorical(probs=chef1_action_probs)\n",
        "            chef1_action = chef1_action_probs_dist.sample()\n",
        "\n",
        "            chef2_action_probs_dist = tfp.distributions.Categorical(probs=chef2_action_probs)\n",
        "            chef2_action = chef2_action_probs_dist.sample()\n",
        "            \n",
        "            '''\n",
        "            chef1_action = np.random.choice(num_actions, p=np.squeeze(chef1_action_probs))\n",
        "            chef2_action = np.random.choice(num_actions, p=np.squeeze(chef2_action_probs))\n",
        "            '''\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            next_observation, reward, done, info = env.step((int(chef1_action), int(chef2_action)))\n",
        "            episode_reward += reward\n",
        "            observation = next_observation\n",
        "\n",
        "            # Lets convert to tensor all we need\n",
        "            chef1_next_observation = next_observation['both_agent_obs'][0]\n",
        "            chef2_next_observation = next_observation['both_agent_obs'][1]\n",
        "\n",
        "            chef1_next_observation = tf.convert_to_tensor([chef1_next_observation], dtype = tf.float32)\n",
        "            chef2_next_observation = tf.convert_to_tensor([chef2_next_observation], dtype = tf.float32)\n",
        "            reward = tf.convert_to_tensor([reward], dtype='float32')\n",
        "\n",
        "            # Predict future rewards from environment state\n",
        "            chef1_next_observation_value = critic(chef1_next_observation)\n",
        "            chef2_next_observation_value = critic(chef2_next_observation)\n",
        "\n",
        "            # To compute the loss we need to get rid of the extra dimenion\n",
        "            #chef1_observation_value = tf.squeeze(chef1_observation_value)\n",
        "            #chef2_observation_value = tf.squeeze(chef2_observation_value)\n",
        "\n",
        "            chef1_next_observation_value = tf.squeeze(chef1_next_observation_value)\n",
        "            chef2_next_observation_value = tf.squeeze(chef2_next_observation_value)\n",
        "\n",
        "            # The advantage function\n",
        "            chef1_target = reward + gamma*chef1_next_observation_value*(1-int(done))\n",
        "            chef2_target = reward + gamma*chef2_next_observation_value*(1-int(done))\n",
        "            \n",
        "            chef1_advantage = chef1_target - chef1_observation_value\n",
        "            chef2_advantage = chef2_target - chef2_observation_value\n",
        "            \n",
        "            '''\n",
        "            chef1_delta = reward + gamma*chef1_next_observation_value*(1-int(done)) - chef1_observation_value\n",
        "            chef2_delta = reward + gamma*chef2_next_observation_value*(1-int(done)) - chef2_observation_value\n",
        "            '''\n",
        "\n",
        "            # Critic loss with MSE loss (==chef_advantage**2)\n",
        "            chef1_critic_loss = mse_loss(chef1_observation_value, chef1_target)\n",
        "            chef2_critic_loss = mse_loss(chef2_observation_value, chef2_target)\n",
        "            '''\n",
        "            chef1_critic_loss = (chef1_advantage**2)\n",
        "            chef2_critic_loss = (chef2_advantage**2)\n",
        "            '''\n",
        "            critic_loss = (0.5)*chef1_critic_loss + (0.5)*chef2_critic_loss \n",
        "\n",
        "            \n",
        "            # Actor loss\n",
        "            # To compute the log probabilities\n",
        "            chef1_log_prob = chef1_action_probs_dist.log_prob(chef1_action)\n",
        "            chef2_log_prob = chef2_action_probs_dist.log_prob(chef2_action)\n",
        "\n",
        "            chef1_actor_loss = -chef1_log_prob*chef1_advantage\n",
        "            chef2_actor_loss = -chef2_log_prob*chef2_advantage\n",
        "            actor_loss = (0.5)*chef1_actor_loss + (0.5)*chef2_actor_loss\n",
        "\n",
        "        # Backpropagation for both Actor & Critic   \n",
        "        actor_grads = tape.gradient(actor_loss, actor.trainable_variables)\n",
        "        critic_grads = tape.gradient(critic_loss, critic.trainable_variables)\n",
        "        \n",
        "        actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
        "        critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
        "\n",
        "        del tape\n",
        "\n",
        "    # Let collect the reward of this episode\n",
        "    reward_histroy.append(episode_reward)\n",
        "    # Update running reward to check condition for solving\n",
        "    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "    # Lets save the losses - We are just saving the last one, not all the losses through out the episode\n",
        "    losses['Actor Loss'].append(actor_loss)\n",
        "    losses['Critic Loss'].append(critic_loss)\n",
        "    \n",
        "    # Log details\n",
        "    if (episode + 1) % 5 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        template2 = \"Actor Loss: {:.2f} - Critic Loss: {:.2f}\"\n",
        "        print(template.format(running_reward, episode), template2.format(tf.squeeze(losses['Actor Loss'][-1]), tf.squeeze(losses['Critic Loss'][-1])))\n",
        "\n",
        "# Plot the reward over the episodes\n",
        "x = [i+1 for i in range(num_episodes)]\n",
        "y = reward_histroy\n",
        "plt.plot(x,y)\n",
        "plt.show\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_episodes = 10\n",
        "n_step_before_update = 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test for Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soup delivered: 0\n"
          ]
        }
      ],
      "source": [
        "import pygame\n",
        "\n",
        "# 1) Initialize Pygame & Visualizer\n",
        "pygame.init()\n",
        "visualizer = StateVisualizer()\n",
        "\n",
        "# 2) Grab your grid and do one dummy render to get a surface\n",
        "grid = base_env.mdp.terrain_mtx\n",
        "_ = env.reset()\n",
        "surf = visualizer.render_state(base_env.state, grid=grid)\n",
        "\n",
        "# 3) Use that surface’s size for your window\n",
        "win_w, win_h = surf.get_size()\n",
        "screen = pygame.display.set_mode((win_w, win_h), pygame.RESIZABLE)\n",
        "clock  = pygame.time.Clock()\n",
        "\n",
        "# 4) Main loop: render each frame & blit into the same window\n",
        "running = True\n",
        "observation = env.reset() #observation of the starting state\n",
        "soup_delivered = 0\n",
        "\n",
        "while running:\n",
        "    for ev in pygame.event.get():\n",
        "        if ev.type == pygame.QUIT:\n",
        "            running = False\n",
        "    \n",
        "    # observation of the environment\n",
        "    chef1_observation = observation['both_agent_obs'][0]\n",
        "    chef2_observation = observation['both_agent_obs'][1]\n",
        "\n",
        "    chef1_observation = keras.ops.convert_to_tensor(chef1_observation)\n",
        "    chef1_observation = keras.ops.expand_dims(chef1_observation, 0)\n",
        "\n",
        "    chef2_observation = keras.ops.convert_to_tensor(chef2_observation)\n",
        "    chef2_observation = keras.ops.expand_dims(chef2_observation, 0)\n",
        "    \n",
        "    # step the environment\n",
        "    chef1_action_probs = actor(chef1_observation)\n",
        "    chef1_action = np.random.choice(num_actions, p=np.squeeze(chef1_action_probs))\n",
        "    \n",
        "    chef2_action_probs = actor(chef2_observation)\n",
        "    chef2_action = np.random.choice(num_actions, p=np.squeeze(chef2_action_probs))\n",
        "\n",
        "    # try to step; if episode is over, catch and reset\n",
        "    try:\n",
        "        # Overcooked wrapper returns (obs_p0, obs_p1, reward, done, info)\n",
        "        observation, reward, done, info = env.step((chef1_action, chef2_action))\n",
        "        if reward > 19:\n",
        "            soup_delivered += 1\n",
        "    except AssertionError:\n",
        "        # base_env.is_done() was True → reset and continue\n",
        "        env.reset()\n",
        "        break\n",
        "\n",
        "    # render the new state\n",
        "    surf = visualizer.render_state(base_env.state, grid=grid)\n",
        "\n",
        "    # draw it\n",
        "    screen.blit(surf, (0, 0))\n",
        "    pygame.display.flip()\n",
        "\n",
        "    clock.tick(15)   # cap at 30 FPS\n",
        "\n",
        "pygame.quit()\n",
        "\n",
        "print(f\"Soup delivered: {soup_delivered}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aas_overcooked",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
